<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shantanu Nitin Ghodgaonkar</title>

  <meta name="author" content="Shantanu Nitin Ghodgaonkar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/friends.png">
  <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon"> -->
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-KX2VVE4DHN"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-KX2VVE4DHN');
  </script>

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    <b>Shantanu Nitin Ghodgaonkar</b>
                  </p>
                  <p>

                    I’m a roboticist passionate about exploring the fascinating intersection
                    of optimal control, reinforcement learning, computer vision, and legged robots. As a Master’s
                    student in Mechatronics, Robotics, and Automation Engineering at NYU, I love diving into
                    complex challenges. Whether it’s designing algorithms for multi-DOF robots, developing
                    vision-based model predictive control systems, or optimizing robot trajectories. With hands-on
                    experience in
                    tools like <b>ROS, PyTorch, and OpenCV</b>, I’m driven by the opportunity to bridge theory and
                    real-world
                    applications, pushing the boundaries of what autonomous systems can achieve.<br><br>
                    My current flagship project is a <b>autonomous Hexapod Robot that integrates SLAM and MPC</b> to
                    navigate dynamic terrains using <b>sensor fusion, real-time path planning, and adaptive control
                      algorithms </b>.


                    <!-- I am a passionate Robotics Software Engineer and a Master’s student in Mechatronics, Robotics, and
                    Automation at NYU Tandon School of Engineering, specializing in advanced Control Engineering. My
                    work focuses on developing innovative robotic systems capable of navigating and adapting to complex
                    environments. From Model Predictive Control (MPC) and Reinforcement Learning to Simultaneous
                    Localization and Mapping (SLAM), I leverage cutting-edge technologies to push the boundaries of
                    automation.  -->


                  </p>
                  <!-- <p>
                    At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>, <a
                      href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a
                      href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a
                      href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">VR</a>, <a
                      href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait
                      Mode</a>, <a
                      href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait
                      Light</a>, and <a href="https://blog.google/products/maps/three-maps-updates-io-2022/">Maps</a>. I
                    did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a
                      href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>. I've received the <a
                      href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
                  </p> -->
                  <p style="text-align:center"><b>
                      <a href="mailto:shantanu.ghodgaonkar@gmail.com">E-Mail</a> &nbsp;/&nbsp;
                      <a href="data/resume/Shantanu_Nitin_Ghodgaonkar_Resume.pdf">Resume</a> &nbsp;/&nbsp;
                      <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                      <a href="https://www.linkedin.com/in/s-n-g/">LinkedIn</a> &nbsp;/&nbsp;
                      <a href="https://github.com/shantanu-ghodgaonkar">GitHub</a>
                    </b></p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/shantanu_picture.jpeg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/shantanu_picture.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2><b>Projects</b></h2>
                  <p>
                    My projects cover topics like Model Predictive Control, Proximal Policy Optimization, Advanced
                    Mechatronics, Internet Of Things, Computer Vision and Embedded Systems.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/hexapod.jpeg' width=100%>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://github.com/shantanu-ghodgaonkar/hexapod">
                    <span class="papertitle">Hexapod Robot: Scalable Trajectory Optimization and Motion Planning</span>
                  </a>
                  <br>
                  <!-- <strong>Shantanu Ghodgaonkar</strong> -->
                  <!-- <br> -->
                  <em>Master’s Project</em>, Fall 2024 - Present
                  <br>
                  <a href="https://github.com/shantanu-ghodgaonkar/hexapod">GitHub Repository</a>
                  <p></p>
                  <p>
                    This project involves developing <strong>scalable trajectory optimization</strong> and
                    <strong>motion
                      planning algorithms</strong>
                    for
                    a 7-DOF hexapod robot, enhancing <b>gait stability</b> and interaction with complex environments.
                  </p>
                </td>
              </tr>



              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/quadrotor.png" alt="Quadrotor RL Target Navigation" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Reinforcement%20Learning%20and%20Optimal%20Control%20for%20Robotics%20(ROB-GY%206323)/Project%202/project2.ipynb">
                    <span class="papertitle">Quadrotor Navigation with Reinforcement Learning</span>
                  </a>
                  <br>
                  <em>Course Project</em>, Fall 2024
                  <br>
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Reinforcement%20Learning%20and%20Optimal%20Control%20for%20Robotics%20(ROB-GY%206323)/Project%202">GitHub
                    Repository</a>
                  <p></p>
                  <p>
                    This project aims to control a 2D quadrotor to reach a target while avoiding obstacles using
                    <b>reinforcement learning</b>. A custom environment is created with the <b>stable_baselines3</b>
                    library, and an
                    RL agent is trained using the <b>Proximal Policy Optimization (PPO)</b> algorithm to navigate
                    efficiently
                    and safely in a simulated environment.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/pendulum.png" alt="Q-Learning for Inverted Pendulum" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Reinforcement%20Learning%20and%20Optimal%20Control%20for%20Robotics%20(ROB-GY%206323)/Homework/Series%204/Series%204%20-%20Q-learning.ipynb">
                    <span class="papertitle"><strong>Q-Learning</strong> with <strong>Neural Networks</strong> for
                      <strong>Inverted Pendulum</strong></span>
                  </a>
                  <br>
                  <em>Course Project</em>, Fall 2024
                  <br>
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Reinforcement%20Learning%20and%20Optimal%20Control%20for%20Robotics%20(ROB-GY%206323)/Homework/Series%204/Series%204%20-%20Q-learning.ipynb">
                    <strong>GitHub Repository</strong>
                  </a>
                  <p></p>
                  <p>
                    This project implements <strong>Q-learning</strong> using a <strong>neural network</strong> to solve
                    the <strong>inverted pendulum</strong> problem.
                    The objective is to train a policy that minimizes a defined <strong>cost function</strong> using
                    <strong>PyTorch</strong>, with
                    <strong>Q-values</strong> approximated by a <strong>neural network</strong>. The project includes
                    the <strong>algorithm implementation</strong>,
                    analysis of performance through <strong>cost</strong> and <strong>value function plots</strong>, and
                    a demonstration video of the
                    <strong>pendulum's behavior</strong>.
                  </p>
                </td>
              </tr>



              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/gridWorld.png" alt="Grid-World Optimal Policy" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Reinforcement%20Learning%20and%20Optimal%20Control%20for%20Robotics%20(ROB-GY%206323)/Homework/Series%203/Exercise2.ipynb">
                    <span class="papertitle"><strong>Grid-World</strong>: <strong>Value Iteration</strong> and
                      <strong>Policy Iteration</strong> for <strong>Optimal Control</strong></span>
                  </a>
                  <br>
                  <em>Course Project</em>, Fall 2024
                  <br>
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Reinforcement%20Learning%20and%20Optimal%20Control%20for%20Robotics%20(ROB-GY%206323)/Homework/Series%203/Exercise2.ipynb">
                    GitHub Repository
                  </a>
                  <p></p>
                  <p>
                    This project explores solving a <strong>Grid-World</strong> problem using <strong>Value
                      Iteration</strong> and <strong>Policy Iteration</strong>
                    algorithms to compute the <strong>Optimal Policy</strong> and <strong>Value Function</strong>. The
                    project compares the
                    <strong>computational complexity</strong> and <strong>convergence behavior</strong> of the two
                    methods, providing insights into their
                    application in real-world <strong>Optimal Control</strong> problems.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/skiptrace.jpg" alt="Skiptrace: Visual Place Recognition for Surveillance"
                      width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Robot%20Perception%20(ROB-GY%206203)/Homework/HW2/task5.py">
                    <span class="papertitle"><strong>Skiptrace</strong>: <strong>Visual Place Recognition</strong> for
                      <strong>Surveillance</strong></span>
                  </a>
                  <br>
                  <em>Course Project</em>, Fall 2024
                  <br>
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Robot%20Perception%20(ROB-GY%206203)/Homework/HW2/task5.py">
                    GitHub Repository
                  </a>
                  <p></p>
                  <p>
                    This project implements a <strong>Visual Place Recognition</strong> system to match query images
                    with a
                    <strong>surveillance photo database</strong> using <strong>feature-based methods</strong>. The
                    approach leverages <strong>SIFT descriptors</strong>,
                    <strong>VLAD encoding</strong>, and <strong>k-means clustering</strong> to efficiently search and
                    identify target locations from a
                    large dataset. The implementation provides a structured pipeline for <strong>feature
                      extraction</strong>, <strong>descriptor management</strong>,
                    <strong>query processing</strong>, and <strong>visualization</strong> of retrieved images.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/object_tracking.jpg" alt="Object Tracking with Optical Flow" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Robot%20Perception%20(ROB-GY%206203)/Homework/HW2/task4.py">
                    <span class="papertitle"><strong>Object Tracking</strong>: <strong>Sparse</strong> and <strong>Dense
                        Optical Flow</strong></span>
                  </a>
                  <br>
                  <em>Course Project</em>, Fall 2024
                  <br>
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Robot%20Perception%20(ROB-GY%206203)/Homework/HW2/task4.py">
                    GitHub Repository
                  </a>
                  <p></p>
                  <p>
                    This project involves implementing <strong>Object Tracking</strong> across video sequences using
                    <strong>Optical Flow</strong>
                    algorithms. Two methods, the <strong>Lucas-Kanade Sparse Optical Flow</strong> and <strong>Farneback
                      Dense Optical Flow</strong>,
                    are explored. The project demonstrates tracking entities persistently, <strong>visualizing motion
                      fields</strong>, and
                    comparing the <strong>efficiency</strong> and <strong>accuracy</strong> of both methods. Detailed
                    step-by-step explanations
                    and results with <strong>bounding box tracking</strong> are provided.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/f_matrix_pose.jpg" alt="F-Matrix and Relative Pose Estimation" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Robot%20Perception%20(ROB-GY%206203)/Homework/HW2/task3.py">
                    <span class="papertitle">F-Matrix and Relative Pose Estimation for Stereo Vision</span>
                  </a>
                  <br>
                  <em>Course Project</em>, 2024
                  <br>
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Robot%20Perception%20(ROB-GY%206203)/Homework/HW2/task3.py">GitHub
                    Repository</a>
                  <p></p>
                  <p>
                    This project computes the <strong>Fundamental Matrix (F-Matrix)</strong> and estimates the
                    <strong>relative pose (R, t)</strong> between two stereo images. Using <strong>Aruco
                      markers</strong> for robust point correspondences, the method applies <strong>RANSAC</strong> to
                    filter outliers and accurately estimate the F-Matrix. The computed epipolar constraints are
                    visualized to validate stereo geometry. The project further derives the <strong>Essential
                      Matrix</strong> and decomposes it to recover the <strong>rotation and translation</strong> between
                    the camera views. Visual results include <strong>epipolar line overlays</strong> on images,
                    providing insights into the quality of estimated parameters.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/quadrotor.png" alt="Quadrotor Looping Maneuver" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/tree/main/Reinforcement%20Learning%20and%20Optimal%20Control%20for%20Robotics%20(ROB-GY%206323)/Project%201/Shantanu_Ghodgaonkar_ROBGY6323_Project1_Report.pdf">
                    <span class="papertitle"><strong>Trajectory Optimization</strong> and <strong>Model Predictive
                        Control</strong> for <strong>2D Quadrotor</strong></span>
                  </a>
                  <br>
                  <em>Course Project</em>, Fall 2024
                  <br>
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/tree/main/Reinforcement%20Learning%20and%20Optimal%20Control%20for%20Robotics%20(ROB-GY%206323)/Project%201">
                    GitHub Repository
                  </a>
                  <p></p>
                  <p>
                    This project focuses on developing a <strong>control system</strong> for a <strong>2D
                      quadrotor</strong> to perform a <strong>looping maneuver</strong>
                    using <strong>optimization-based techniques</strong>. It includes <strong>trajectory
                      optimization</strong> via <strong>Sequential Quadratic
                      Programming (SQP)</strong> and extends to <strong>Model Predictive Control (MPC)</strong> for
                    real-time adaptability and
                    robustness in <strong>simulated environments</strong>.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/icp_point_cloud.jpg" alt="ICP-Based 3D Point Cloud Alignment" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Robot%20Perception%20(ROB-GY%206203)/Homework/HW2/task2b.py">
                    <span class="papertitle">ICP-Based 3D Point Cloud Alignment Using Open3D</span>
                  </a>
                  <br>
                  <em>Course Project</em>, Fall 2024
                  <br>
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Robot%20Perception%20(ROB-GY%206203)/Homework/HW2/task2b.py">GitHub
                    Repository</a>
                  <p></p>
                  <p>
                    This project implements the <strong>Iterative Closest Point (ICP)</strong> algorithm to align 3D
                    point clouds, a crucial step in <strong>point cloud registration</strong>. It aligns two datasets by
                    iteratively refining transformation estimates based on correspondences. The project includes two
                    parts: one focusing on Open3D's demo point clouds and another applying ICP to <strong>KITTI
                      self-driving dataset</strong>. The methodology involves <strong>correspondence matching, centroid
                      alignment, rotation estimation using Singular Value Decomposition (SVD), and error convergence
                      analysis</strong>. The results compare the accuracy of alignment between different datasets,
                    highlighting ICP’s strengths and limitations.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/ransac_plane_fitting.jpg" alt="RANSAC Plane Fitting" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Robot%20Perception%20(ROB-GY%206203)/Homework/HW2/task1.py">
                    <span class="papertitle">RANSAC-Based 3D Plane Fitting for Point Cloud Data</span>
                  </a>
                  <br>
                  <em>Course Project</em>, Fall 2024
                  <br>
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Robot%20Perception%20(ROB-GY%206203)/Homework/HW2/task1.py">GitHub
                    Repository</a>
                  <p></p>
                  <p>
                    This project implements <strong>RANSAC (Random Sample Consensus)</strong> for fitting a plane to 3D
                    point cloud data. The algorithm robustly estimates plane parameters while handling
                    <strong>outliers</strong>, making it a valuable tool in <strong>computer vision and
                      robotics</strong>. The implementation follows an iterative approach with inlier classification and
                    adaptive sampling to optimize computation. Using <strong>Open3D</strong>, the results visualize
                    fitted planes where inliers are marked in <strong>red</strong> and outliers in
                    <strong>green</strong>, effectively identifying dominant planar structures in 3D space.
                  </p>
                </td>
              </tr>



              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/pendulum.png" alt="SQP for Nonlinear Optimal Control" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Reinforcement%20Learning%20and%20Optimal%20Control%20for%20Robotics%20(ROB-GY%206323)/Homework/Series%202/exercise%203_pendulum.ipynb">
                    <span class="papertitle"><strong>Implementation of SQP</strong> for <strong>Nonlinear Optimal
                        Control</strong></span>
                  </a>
                  <br>
                  <em>Course Project</em>, Fall 2024
                  <br>
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Reinforcement%20Learning%20and%20Optimal%20Control%20for%20Robotics%20(ROB-GY%206323)/Homework/Series%202/exercise%203_pendulum.ipynb">
                    GitHub Repository
                  </a>
                  <p></p>
                  <p>
                    This project demonstrates the implementation of a <strong>Sequential Quadratic Programming
                      (SQP)</strong> solver to
                    solve a <strong>nonlinear optimal control</strong> problem. The problem focuses on controlling the
                    dynamics of a
                    <strong>pendulum</strong> to achieve a target configuration with <strong>minimal energy</strong> and
                    <strong>precise control</strong>, using
                    <strong>discretized dynamics</strong> and <strong>optimization techniques</strong>.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/tag_based_ar.jpg" alt="Tag-based Augmented Reality" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/blob/main/Robot%20Perception%20(ROB-GY%206203)/Homework/HW1/Q4/q4_ans.py">
                    <span class="papertitle"><strong>Tag-based Augmented Reality</strong>: <strong>AR Cube
                        Projection</strong> on <strong>ArUco Markers</strong></span>
                  </a>
                  <br>
                  <em>Course Project</em>, Fall 2024
                  <br>
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL24/tree/main/Robot%20Perception%20(ROB-GY%206203)/Homework/HW1/Q4">
                    GitHub Repository
                  </a>
                  <p></p>
                  <p>
                    This project demonstrates the implementation of <strong>tag-based augmented reality</strong> using
                    <strong>ArUco markers</strong>. The
                    program detects tags, <strong>estimates their pose</strong>, and overlays <strong>3D
                      objects</strong> such as a cube on the detected
                    tags. It leverages <strong>OpenCV</strong> for <strong>marker detection</strong>, <strong>pose
                      estimation</strong>, and <strong>visualization</strong>, showcasing
                    augmented reality results from multiple perspectives. The implementation involves <strong>intrinsic
                      camera calibration</strong>,
                    <strong>marker size definitions</strong>, and <strong>detailed pose transformations</strong> for
                    accurate <strong>AR projections</strong>.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/solution_path.png" alt="Maze Solving Robot" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://github.com/shantanu-ghodgaonkar/rPiUnoMazeSolver">
                    <span class="papertitle"><strong>Maze Solving Robot</strong></span>
                  </a>
                  <br>
                  <em>Course Project</em>, Spring 2024
                  <br>
                  <a href="https://github.com/shantanu-ghodgaonkar/rPiUnoMazeSolver">
                    GitHub Repository
                  </a>
                  <p></p>
                  <p>
                    This project showcases a <strong>maze-solving robot</strong> developed using <strong>Raspberry Pi
                      4B</strong> and <strong>Arduino Uno</strong>
                    microcontrollers. By leveraging <strong>OpenCV</strong> for <strong>image processing</strong>, the
                    robot identifies the ball's
                    position and <strong>maze walls</strong>, while the <strong>breadth-first search (BFS)
                      algorithm</strong> calculates the <strong>optimal path</strong>
                    to the maze's exit. <strong>Servo motors</strong> controlled by the <strong>Arduino</strong> execute
                    the navigation commands, enabling
                    the robot to manipulate the ball’s movement effectively. The system operates in
                    <strong>real-time</strong>,
                    dynamically recalculating the ball’s path as it navigates through the maze.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/vision_imu_ukf.jpg" alt="Vision and IMU Fusion with Unscented Kalman Filter"
                      width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://github.com/shantanu-ghodgaonkar/Proj3-ROBGY6213/tree/main">
                    <span class="papertitle"><strong>Vision and IMU Fusion</strong> with <strong>Unscented Kalman Filter
                        (UKF)</strong></span>
                  </a>
                  <br>
                  <em>Course Project</em>, Spring 2024
                  <br>
                  <a href="https://github.com/shantanu-ghodgaonkar/Proj3-ROBGY6213/tree/main">
                    GitHub Repository
                  </a>
                  <p></p>
                  <p>
                    This project explores the use of an <strong>Unscented Kalman Filter (UKF)</strong> for
                    <strong>sensor fusion</strong>, integrating
                    data from an <strong>Inertial Measurement Unit (IMU)</strong> and a <strong>vision-based
                      system</strong> for enhanced <strong>robot state estimation</strong>.
                    The <strong>UKF</strong> is selected for its ability to handle <strong>nonlinearities</strong>,
                    making it more accurate than traditional
                    <strong>Kalman Filters</strong>. Two scenarios are evaluated: using <strong>visual pose
                      estimation</strong> for position and orientation
                    measurements, and employing <strong>optical flow-derived velocity measurements</strong>. Performance
                    is analyzed by comparing
                    the estimated trajectories with <strong>ground-truth sensor data</strong>, demonstrating the
                    effectiveness of <strong>UKF</strong> in handling
                    <strong>nonlinear system models</strong> and limited measurements.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/conveyerautosep.jpg" alt="Automatic Package Separator" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://github.com/shantanu-ghodgaonkar/conveyerAutoSep">
                    <span class="papertitle"><strong>Automatic Package Separator</strong></span>
                  </a>
                  <br>
                  <em>Course Project</em>, Spring 2024
                  <br>
                  <a href="https://github.com/shantanu-ghodgaonkar/conveyerAutoSep">
                    GitHub Repository
                  </a>
                  <p></p>
                  <p>
                    This project introduces an <strong>Automatic Package Separator</strong> system using
                    <strong>Arduino</strong> technology to enhance
                    the efficiency of <strong>package handling</strong> and <strong>segregation</strong>. The system
                    employs <strong>1D/2D barcode scanning</strong> for
                    <strong>address detection</strong>, <strong>servo motors</strong> for package redirection, and an
                    <strong>LCD module</strong> for real-time feedback.
                    Designed for <strong>logistics</strong> and <strong>supply chain management</strong>, this
                    innovative solution optimizes workflows and
                    improves <strong>package processing accuracy</strong>.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/robotic_kinematics.jpg" alt="Robotic Kinematics and Visualization" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL23/tree/main/Foundations%20of%20Robotics%20(ROB-GY%206003)/Project/Final%20Files">
                    <span class="papertitle"><strong>Robotic Kinematics and Visualization</strong></span>
                  </a>
                  <br>
                  <em>Course Project</em>, Fall 2023
                  <br>
                  <a
                    href="https://github.com/shantanu-ghodgaonkar/NYUMSMRFALL23/tree/main/Foundations%20of%20Robotics%20(ROB-GY%206003)/Project/Final%20Files">
                    GitHub Repository
                  </a>
                  <p></p>
                  <p>
                    This project focuses on implementing <strong>robotic forward and inverse kinematics</strong> and
                    visualizing the
                    results through a "<strong>connect-the-dots</strong>" <strong>stick-figure representation</strong>
                    of a robot. The platform emphasizes
                    <strong>accurate computation</strong> of <strong>joint positions</strong> and <strong>end-effector
                      configurations</strong> for <strong>complex robotic systems</strong>,
                    as well as <strong>error handling</strong> when desired configurations are unreachable. The logic
                    includes
                    detailed functionality for <strong>forward kinematics</strong>, <strong>inverse kinematics</strong>,
                    and <strong>visualization</strong>.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src="images/wpmscdp.png" alt="Wearable Physiological Monitoring System" width="100%">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://github.com/shantanu-ghodgaonkar/WPMSCDP">
                    <span class="papertitle"><strong>Wearable Physiological Monitoring System</strong> Using
                      <strong>ClusterDuck Protocol</strong></span>
                  </a>
                  <br>
                  <em>Bachelor's Project</em>, 2017
                  <br>
                  <a href="https://github.com/shantanu-ghodgaonkar/WPMSCDP">
                    GitHub Repository
                  </a>
                  <p></p>
                  <p>
                    This project focuses on the development of a <strong>Wearable Physiological Monitoring
                      System</strong> that
                    leverages the <strong>ClusterDuck Protocol (CDP)</strong> to enable <strong>reliable
                      communication</strong> between <strong>wearable health monitoring devices</strong>
                    and a base station. Designed for <strong>real-time monitoring</strong>, the system measures
                    key <strong>physiological parameters</strong>, such as <strong>heart rate</strong> and <strong>body
                      temperature</strong>, and transmits the data
                    wirelessly using <strong>LoRa technology</strong>. The wearable device is built with an
                    <strong>ESP32 microcontroller</strong>,
                    ensuring <strong>low power consumption</strong> and seamless communication, even in challenging
                    conditions or emergencies.
                  </p>
                </td>
              </tr>




            </tbody>
          </table>


          <!-- <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">
            <tbody>
              <tr>
                <td>
                  <h2>Miscellanea</h2>
                </td>
              </tr>
            </tbody>
          </table> -->
          <!-- <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #fcb97d;">
                    <h2>Micropapers</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                  <br>
                  <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain
                    Functions</a>
                  <br>
                  <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                  <br>
                  <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How
                    Can Academics Adapt?</a>
                </td>
              </tr>


              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #aaba9e;">
                    <h2>Recorded Talks</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:center">
                  <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a>
                  <br>
                  <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
                  </a>
                  <br>
                  <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a>
                  <br>
                  <a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a>
                  <br>
                  <a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
                </td>
              </tr>

              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #c6b89e;">
                    <h2>Academic Service</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:center">
                  <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Area Chair, CVPR 2025</a>
                  <br>
                  <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                  <br>
                  <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                  <br>
                  <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                  <br>
                  <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                  <br>
                  <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                  <br>
                  <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
                </td>
              </tr>



              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #edd892;">
                    <h2>Teaching</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:center">
                  <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor,
                    CS188 Spring 2011</a>
                  <br>
                  <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor,
                    CS188 Fall 2010</a>
                  <br>
                  <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd
                    Edition</a>
                </td>
              </tr>

            </tbody>
          </table> -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source
                      code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics
                    tags that you do not want on your own website &mdash; use the github code instead. Also, consider
                    using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a
                      href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>